{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems 2020/21\n",
    "\n",
    "\n",
    "## Practice 4 - Building an ItemKNN Recommender From Scratch\n",
    "\n",
    "This practice session is created to provide a guide to students of how to crete a recommender system from scratch, going from the data loading, processing, model creation, evaluation, hyperparameter tuning and a sample submission to the competition. \n",
    "\n",
    "Outline:\n",
    "- Data Loading with Pandas (MovieLens 10M, link: http://files.grouplens.org/datasets/movielens/ml-10m.zip)\n",
    "- Data Preprocessing\n",
    "- Dataset splitting in Train, Validation and Testing\n",
    "- Similarity Measures\n",
    "- Collaborative Item KNN\n",
    "- Evaluation Metrics\n",
    "- Evaluation Procedure\n",
    "- Hyperparameter Tuning\n",
    "- Submission to competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'Fernando Benjamín Pérez Maurera'\n",
    "__credits__ = ['Fernando Benjamín Pérez Maurera']\n",
    "__license__ = 'MIT'\n",
    "__version__ = '0.1.0'\n",
    "__maintainer__ = 'Fernando Benjamín Pérez Maurera'\n",
    "__email__ = 'fernandobenjamin.perez@polimi.it'\n",
    "__status__ = 'Dev'\n",
    "\n",
    "import os\n",
    "from typing import Tuple, Callable, Dict, Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading with pandas\n",
    "\n",
    "The Movielens 10M dataset is a collection of ratings given by users to items. They are stored in a columnar `.dat` file using `::` as separators for each attribute, and every row follows this structure: `<user_id>::<item_id>::<rating>::<timestamp>`. \n",
    "\n",
    "The function `read_csv` from pandas provides a wonderful and fast interface to load tabular data like this. For better results and performance we provide the separator `::`, the column names `[\"user_id\", \"item_id\", \"ratings\", \"timestamp\"]`, and the types of each attribute in the `dtype` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "This section wors with the previously-loaded ratings dataset and extracts the number of users, number of items, and min/max user/item identifiers. Exploring and understanding the data is an essential step prior fitting any recommender/algorithm. \n",
    "\n",
    "In this specific case, we discover that item identifiers go between 1 and 65133, however, there are only 10677 different items (meaning that ~5/6 of the items identifiers are not present in the dataset). To ease further calculations, we create new contiguous user/item identifiers, we then assign each user/item only one of these new identifiers. To keep track of these new mappings, we add them into the original dataframe using the `pd.merge` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting into Train, Validation, and Test\n",
    "\n",
    "This is the last part before creating the recommender. However, this step is super *important*, as it is the base for the training, parameters optimization, and evaluation of the recommender(s).\n",
    "\n",
    "In here we read the ratings (which we loaded and preprocessed before) and create the `train`, `validation`, and `test` User-Rating Matrices (URM). It's important that these are disjoint to avoid information leakage from the train into the validation/test set, in our case, we are safe to use the `train_test_split` function from `scikit-learn` as the dataset only contains *one* datapoint for every `(user,item)` pair. On another topic, we first create the `test` set and then we create the `validation` by splitting again the `train` set.\n",
    "\n",
    "\n",
    "`train_test_split` takes an array (or several arrays) and divides it into `train` and `test` according to a given size (in our case `testing_percentage` and `validation_percentage`, which need to be a float between 0 and 1).\n",
    "\n",
    "After we have our different splits, we create the *sparse URMs* by using the `csr_matrix` function from `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "We can implement different versions of a cosine similarity. Some of these are faster and others are slower.\n",
    "\n",
    "The most simple version is just to loop item by item and calculate the similarity of item pairs.\n",
    "$$ W_{i,j} \n",
    "= cos(v_i, v_j) \n",
    "= \\frac{v_i \\cdot v_j}{|| v_i || ||v_j ||} \n",
    "= \\frac{\\Sigma_{u \\in U}{URM_{u,i} \\cdot URM_{u,j}}}{\\sqrt{\\Sigma_{u \\in U}{URM_{u,i}^2}} \\cdot \\sqrt{\\Sigma_{u \\in U}{URM_{u,j}^2}} + shrink} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another (faster) version of the similarity is by operating on vector products\n",
    "$$ W_{i,I} \n",
    "= cos(v_i, URM_{I}) \n",
    "= \\frac{v_i \\cdot URM_{I}}{|| v_i || IW_{I} + shrink} $$\n",
    "\n",
    "and where \n",
    "\n",
    "$$ IW_{i} = \\sqrt{{\\Sigma_{u \\in U}{URM_{u,i}^2}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, a faster but more memory-intensive version of the similarity is by operating on matrix products\n",
    "$$ W  \n",
    "= \\frac{URM^{t} \\cdot URM}{IW^{t} IW + shrink} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering ItemKNN Recommender\n",
    "\n",
    "This step creates a `CFItemKNN` class that represents a Collaborative Filtering ItemKNN Recommender. As we have mentioned in previous practice sessions, our recommenders have two main functions: `fit` and `recommend`. \n",
    "\n",
    "The first receives the similarity function and the dataset with which it will create the similarities, the result of this function is to save the similarities (`weights`) into the class instance. \n",
    "\n",
    "The second function takes a user id, the train URM, the recommendation lenght and a boolean value to remove already-seen items from users. It returns a recommendation list for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "In this practice session we will be using the same evaluation metrics defined in the Practice session 2, i.e., precision, recall and mean average precision (MAP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(recommendations: np.array, relevant_items: np.array) -> float:\n",
    "    is_relevant = np.in1d(recommendations, relevant_items, assume_unique=True)\n",
    "    \n",
    "    recall_score = np.sum(is_relevant) / relevant_items.shape[0]\n",
    "    \n",
    "    return recall_score\n",
    "    \n",
    "    \n",
    "def precision(recommendations: np.array, relevant_items: np.array) -> float:\n",
    "    is_relevant = np.in1d(recommendations, relevant_items, assume_unique=True)\n",
    "    \n",
    "    precision_score = np.sum(is_relevant) / recommendations.shape[0]\n",
    "\n",
    "    return precision_score\n",
    "\n",
    "def mean_average_precision(recommendations: np.array, relevant_items: np.array) -> float:\n",
    "    is_relevant = np.in1d(recommendations, relevant_items, assume_unique=True)\n",
    "    \n",
    "    precision_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "\n",
    "    map_score = np.sum(precision_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Procedure\n",
    "\n",
    "The evaluation procedure returns the averaged accuracy scores (in terms of precision, recall and MAP) for all users (that have at least 1 rating in the test set). It also calculates the number of evaluated and skipped users. It receives a recommender instance, and the train and test URMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "This step is fundamental to get the best performance of an algorithm, specifically, because we will train different configurations of the parameters for the `CFItemKNN` recommender and select the best performing one.\n",
    "\n",
    "In order for this step to be meaningful (and to avoid overfitting on the test set), we perform it using the `validation` URM as test set.\n",
    "\n",
    "This step is the longest one to run in the entire pipeline when building a recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission to competition\n",
    "\n",
    "This step serves as a similar step that you will perform when preparing a submission to the competition. Specially after you have chosen and trained your recommender.\n",
    "\n",
    "For this step the best suggestion is to select the most-performing configuration obtained in the hyperparameter tuning step and to train the recommender using both the `train` and `validation` set. Remember that in the competition you *do not* have access to the test set.\n",
    "\n",
    "We simulated the users to generate recommendations by randomly selecting 100 users from the original identifiers. Do consider that in the competition you are most likely to be provided with the list of users to generate recommendations. \n",
    "\n",
    "Another consideration is that, due to easier and faster calculations, we replaced the user/item identifiers with new ones in the preprocessing step. For the competition, you are required to generate recommendations using the dataset's original identifiers. Due to this, this step also reverts back the newer identifiers with the ones originally found in the dataset.\n",
    "\n",
    "Last, this step creates a function that writes the recommendations for each user in the same file in a tabular format following this format: \n",
    "```csv\n",
    "<user_id>,<item_id_1> <item_id_2> <item_id_3> <item_id_4> <item_id_5> <item_id_6> <item_id_7> <item_id_8> <item_id_9> <item_id_10>\n",
    "```\n",
    "\n",
    "Always verify the competitions' submission file model as it might vary from the one we presented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture we saw the most simple version of Cosine Similarity, where it just includes a shrink factor. There are different optimizations that we can do to it.\n",
    "\n",
    "- Implement TopK Neighbors\n",
    "- When calculating the cosine similarity we used `urm.T.dot(urm)` to calculate the enumerator. However, depending of the dataset and the number of items, this matrix could not fit in memory. Implemenent a `block` version, faster than our `vector` version but that does not use `urm.T.dot(urm)` beforehand.\n",
    "- Implement Adjusted Cosine [Formula link](http://www10.org/cdrom/papers/519/node14.html)\n",
    "- Implement Dice Similarity [Wikipedia Link](https://en.wikipedia.org/wiki/Sørensen–Dice_coefficient)\n",
    "- Implement an implicit CF ItemKNN.\n",
    "- Implement a CF UserKNN model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
